.... [[3. Обработка данных 16S]]

+ Формирование гипотезы
	+ $H_0$ - отличие различий
	+ $H_A$ -вопрос исследователя формируется до экпсеримента
+ Гипотеза 
	+ Односторонняя
	+ Двусторонняя

# Метагеномные данные
Объекты могут описываться множеством признаков
+ Сообщество - виды
+ Морфометрические признаки - размеры тех или иных частей
+ Поведенческая (Социальная активность животных) - проявление того или иного паттерна
+ .
**Объект = семпл = образец, который изучаем**
**Свойства объектов - то с чем мы работаем = фичи** 
	Позволяют сгруппировать объекты
## Многомерные данные 
+ В виде таблицы

|            | Ген 1 | Ген 2 | ... | Ген 200 |
| ---------- | ----- | ----- | --- | ------- |
| Пациент 1  |       |       |     |         |
| Пациент 2  |       |       |     |         |
| ....       |       |       |     |         |
| Пациент 20 |       |       |     |         |
+ Ошибка первого и второго рода
	+ Чем больше вероятность ошибки первого рода ($\alpha$), тем меньше второго
	+ При разных исследованиях нужно уменьшать либо $\alpha$ либо $\beta$, например:
		+ При ковиде - лучше уменьшать $\beta$
		+ При раке - лучше уменьшать $\alpha$

**Мощность теста** - насколько мы уверенны, что результат истинно верный
+ Не менее 0.8
+ 1 - $\beta$ (1 - ошибка второго рода)
## Представим что табличек много, а не одна
+ Метаданные
	+ Кровь
	+ Протеом
	+ МРТ....
+ ***Сложно найти референсную здоровую группу***
## МНОГОМЕРНАЯ СТАТИСТИКА
+ Выявление взаимоотношений (сходства-различия) между объектами (или признаками)
+ Здоров или болен? Классификация
+ Ординация
	+ nMDS, PCA
+ Тестирование гипотез о различиях между группами
	+ ANOSIM, PERMANOVA
+ Выявление связи между группами признаков
	+ тест Мантела
	+ BIOENV
	+ RDA
	+ CCA
**R-analysis** - Взаимоотношения между признаками
**Q-analysis** - Взаимоотношения между объектами

>[!important]
>**Часто интересно взаиморасположение точек в многомерном пространстве, а не абсолютные координаты**
### КАК? Способы:
1. Линейная алгебра
	1. Нужна матрица косинусов
	2. и ряд длин векторов
	+ PCA
	+ CA
	+ CCA
	+ RDA
3. Матрица расстояний - представления взаиморасположения лежит в основе 
	+ иерархического кластерного анализа
	+ permanova
	+ anosim, etc.
### Similarity/dissimilarity matrix
1. Евклидово расстояние
	1. Матрица с 0 по диагонали, симметричная и квадратная
2. ***Много других***!!!
#### Метрики-неметрики
1. Метрические коэф.
	1. Неравеснтво треугольника
	2. Симметричность
	+ $a = b \rightarrow D(a,b) = 0$
	+ $D(a,b) = D(b,a)$
	+ Неравенство треугольника $D(a,b) + D(b,c) \geq D(a,c)$
1. Неметрические ^15d404
	1. Bray-Curtis distance
	2. Unifrac distance
	3. Бинарные - Jaccar distance/coefficient
## Проблемы с данными
1. Пропущенные значения - потеряли, прошляпили, человеческий фактор
	+ Импутация - но плохо, потому что придумываете данные на основе алгоритма + доп.шум
2. Разнородность шкал
	+ Стандартизация - можно сравнивать между собой
	+ Нормализация
	+ Трансформация данных - когда данные ненормальные
3. Обилия (abundance) - надо сбалансировать
	+ Перевод в  относительные велечины (доли от суммы)
4. Веса признаков - транскрипты генов домашнего хозяйства - у них всегда высокий уровень экспрессии
	+ Трансформация

> [!important]
> Часто возникает ситуация, когда один признак (или несколько признаков) имеет существенно более высокие абсолютные значения, чем все остальные, или варьирует в более широких пределах, чем остальные признаки. В такой ситуации необходима трансформация , которая “уравнивает” силу влияния признаков

***Двойные нули***
1. Никак не считать
	1. Манхеттен
2. Считать за сходство (отсутствие вида, как сходство)
	1. Брей-кёртис
	2. Евклид
## Ординация
### В сокращенном пространстве признаков:
1. MDS
2. nMDS
3. PCA
> После ординации на нее накладывют информацию о внешних факторах

**Собственны вектора** - перпендикуляры друг другу - ортогональны и НЗ
+ Задают главные компоненты - направление новых осей
+ Это линейные комбинации исходных признаков
**Собственные числа** - показывают **дисперсию** вдоль осей, заданных собственными векторами
+ Первый собств вектор соответсвует наибольшему собственному числу
### Сколько PC взять?
+ Правило Кайзера-Гатмана - взять те, у которых собств.число > 1
+ В сумме объянсяют заданный % от общей изменчивости (60-80%)
+ Broken-stick model (правило колена)

## nMDS
Популярен, потому что
1. Можно иметь пропущенные значения
2. Можно использовать любую матрицу расстояний
3. Можно использовать как количественные, так и категориальные признаки
**НО является итеративным алгоритмом, а значит требователен к вычислительным ресурсам**
